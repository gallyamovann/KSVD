{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXI-w8CF9N-N",
    "outputId": "ac406122-4fa6-4088-889c-ade51812c3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorly in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorly) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorly) (1.23.5)\n",
      "Requirement already satisfied: nose in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorly) (1.3.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorly\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3.1 from C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip (python 3.9)\n",
      "Collecting tensorly==0.7.0\n",
      "  Using cached tensorly-0.7.0-py3-none-any.whl (198 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Collecting nose\n",
      "  Using cached nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "Installing collected packages: nose, numpy, scipy, tensorly\n",
      "  Attempting uninstall: nose\n",
      "    Found existing installation: nose 1.3.7\n",
      "    Uninstalling nose-1.3.7:\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nose-1.3.7.dist-info\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nose\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\man\\man1\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\scripts\\nosetests-3.4.exe\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\scripts\\nosetests.exe\n",
      "      Successfully uninstalled nose-1.3.7\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy-1.23.5.dist-info\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\scripts\\f2py.exe\n",
      "      Successfully uninstalled numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError.\n",
      "Consider using the `--user` option or check the permissions.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 494, in run\n",
      "    installed = install_given_reqs(\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 90, in install_given_reqs\n",
      "    uninstalled_pathset.commit()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 424, in commit\n",
      "    self._moved_paths.commit()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 277, in commit\n",
      "    save_dir.cleanup()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\utils\\temp_dir.py\", line 173, in cleanup\n",
      "    rmtree(self._path)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 328, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 408, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 364, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 197, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\tenacity\\__init__.py\", line 411, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 128, in rmtree\n",
      "    shutil.rmtree(dir, ignore_errors=ignore_errors, onerror=rmtree_errorhandler)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\shutil.py\", line 624, in _rmtree_unsafe\n",
      "    _rmtree_unsafe(fullname, onerror)\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\shutil.py\", line 629, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\shutil.py\", line 627, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] \\u041e\\u0442\\u043a\\u0430\\u0437\\u0430\\u043d\\u043e \\u0432 \\u0434\\u043e\\u0441\\u0442\\u0443\\u043f\\u0435: 'C:\\\\Users\\\\nelli\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\Lib\\\\site-packages\\\\~6mpy\\\\.libs\\\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll'\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall -v \"tensorly==0.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3.1 from C:\\Users\\nelli\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip (python 3.9)\n",
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy-1.24.3.dist-info\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\\n",
      "      Removing file or directory c:\\users\\nelli\\appdata\\local\\programs\\python\\python39\\scripts\\f2py.exe\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 22.9.24 which is incompatible.\n",
      "tensorflow 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-gpu 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 22.9.24 which is incompatible.\n",
      "tensorflow-gpu 2.9.1 requires tensorflow-estimator<2.10.0,>=2.9.0rc0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "numba 0.55.2 requires numpy<1.23,>=1.18, but you have numpy 1.23.5 which is incompatible.\n",
      "flwr 0.19.0 requires numpy<1.21.0,>=1.19.0; python_version < \"3.10\", but you have numpy 1.23.5 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall -v \"numpy==1.23.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Type\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.misc\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import tensorly as tl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import orthogonal_mp\n",
    "from tensorly.decomposition import tucker, partial_tucker\n",
    "from sklearn.feature_extraction.image import extract_patches_2d, reconstruct_from_patches_2d\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pursuit:\n",
    "    \"\"\"\n",
    "    Algorithms that inherit from this class are methods to solve problems of the like\n",
    "    \\min_A \\| DA - Y \\|_2 s.t. \\|A\\|_0 <= t.\n",
    "    Here, D is a given dictionary of size (n x K)\n",
    "    Y is a given matrix of size (n x N), where N is the number of samples\n",
    "    The Pursuit will return a matrix A of size (K x N).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary, max_iter=False, tol=None, sparsity=None):\n",
    "        self.D = np.array(dictionary.matrix)\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.sparsity = sparsity\n",
    "        if (self.tol is None and self.sparsity is None) or (self.tol is not None and self.sparsity is not None):\n",
    "            raise ValueError(\"blub\")\n",
    "        self.data = None\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, Y):\n",
    "        return [], self.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalMatchingPursuit(Pursuit):\n",
    "    \"\"\"\n",
    "    Wrapper for orthogonal_mp from scikit-learn\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, Y):\n",
    "        return orthogonal_mp(self.D, Y, n_nonzero_coefs=self.sparsity,\n",
    "                             tol=self.tol, precompute=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    \"\"\"\n",
    "    The Dictionary class is more or less a wrapper around the numpy array class. It holds a numpy ndarray in\n",
    "    the attribute `matrix` and adds some useful functions for it. The dictionary elements can be accessed\n",
    "    either by D.matrix[i,j] or directly through D[i,j].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = np.array(matrix)\n",
    "        self.shape = matrix.shape\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.matrix[item]\n",
    "\n",
    "    def is_unitary(self):\n",
    "        \"\"\"\n",
    "        Checks whether the dictionary is unitary.\n",
    "        Returns:\n",
    "            True, if the dicitonary is unitary.\n",
    "        \"\"\"\n",
    "        n, K = self.shape\n",
    "        if n == K:\n",
    "            return np.allclose(np.dot(self.matrix.T, self.matrix), np.eye(n))\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_normalized(self):\n",
    "        \"\"\"\n",
    "        Checks wheter the dictionary is l2-normalized.\n",
    "        Returns:\n",
    "            True, if dictionary is l2-normalized.\n",
    "        \"\"\"\n",
    "        n, K = self.shape\n",
    "        return np.allclose([np.linalg.norm(self.matrix[:, i]) for i in range(K)], np.ones(K))\n",
    "\n",
    "\n",
    "    def mutual_coherence(self):\n",
    "        \"\"\"\n",
    "        Computes the dictionary's mutual coherence.\n",
    "        Returns:\n",
    "            Mutual coherence\n",
    "        \"\"\"\n",
    "        return np.max(self._mutual_coherence(self.matrix))\n",
    "\n",
    "    @staticmethod\n",
    "    def _mutual_coherence(D):\n",
    "        n, K = D.shape\n",
    "        mu = [np.abs(np.dot(D[:, i].T, D[:, j]) /\n",
    "                     (np.linalg.norm(D[:, i]) * np.linalg.norm(D[:, j])))\n",
    "              for i in range(K) for j in range(K) if j != i]\n",
    "        return mu\n",
    "\n",
    "    def to_img(self):\n",
    "        \"\"\"\n",
    "        Transforms the dictionary columns into patches and orders them for plotting purposes.\n",
    "        Returns:\n",
    "            Reordered dictionary matrix\n",
    "        \"\"\"\n",
    "        # dictionary dimensions\n",
    "        D = self.matrix\n",
    "        n, K = D.shape\n",
    "        M = self.matrix\n",
    "        # stretch atoms\n",
    "        for k in range(K):\n",
    "            M[:, k] = M[:, k] - (M[:, k].min())\n",
    "            if M[:, k].max():\n",
    "                M[:, k] = M[:, k] / D[:, k].max()\n",
    "\n",
    "        # patch size\n",
    "        n_r = int(np.sqrt(n))\n",
    "\n",
    "        # patches per row / column\n",
    "        K_r = int(np.sqrt(K))\n",
    "\n",
    "        # we need n_r*K_r+K_r+1 pixels in each direction\n",
    "        dim = n_r * K_r + K_r + 1\n",
    "        V = np.ones((dim, dim)) * np.min(D)\n",
    "\n",
    "        # compute the patches\n",
    "        patches = [np.reshape(D[:, i], (n_r, n_r)) for i in range(K)]\n",
    "\n",
    "        # place patches\n",
    "        for i in range(K_r):\n",
    "            for j in range(K_r):\n",
    "                V[j * n_r + 1 + j:(j + 1) * n_r + 1 + j, i * n_r + 1 + i:(i + 1) * n_r + 1 + i] = patches[\n",
    "                    i * K_r + j]\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSVD:\n",
    "    \"\"\"\n",
    "    Implements the original K-SVD Algorithm as described in [1].\n",
    "    [1] Aharon, M., Elad, M. and Bruckstein, A., 2006. K-SVD: An algorithm for designing overcomplete dictionaries for\n",
    "        sparse representation. IEEE Transactions on signal processing, 54(11), p.4311.\n",
    "    Args:\n",
    "        dictionary: Initial dictionary of type sparselandtools.dictionaries.Dictionary\n",
    "        pursuit: Pursuit method to be used (any method from sparselandtools.pursuits)\n",
    "        sparsity: Target sparsity\n",
    "        noise_gain: Target noise_gain. If set, this will override the target sparsity\n",
    "        sigma: Signal or image noise standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary: Dictionary, pursuit: Type[Pursuit], sparsity: int, noise_gain=None, sigma=None):\n",
    "        self.dictionary = Dictionary(dictionary.matrix)\n",
    "        self.alphas = None\n",
    "        self.pursuit = pursuit\n",
    "        self.sparsity = sparsity\n",
    "        self.noise_gain = noise_gain\n",
    "        self.sigma = sigma\n",
    "        self.original_image = None\n",
    "        self.sparsity_values = []\n",
    "        self.mses = []\n",
    "        self.ssims = []\n",
    "        self.psnrs = []\n",
    "        self.iter = None\n",
    "\n",
    "    def sparse_coding(self, Y: np.ndarray):\n",
    "        logging.info(\"Entering sparse coding stage...\")\n",
    "        if self.noise_gain and self.sigma:\n",
    "            p = self.pursuit(self.dictionary, tol=(self.noise_gain * self.sigma))\n",
    "        else:\n",
    "            p = self.pursuit(self.dictionary, sparsity=self.sparsity)\n",
    "        self.alphas = p.fit(Y)\n",
    "        logging.info(\"Sparse coding stage ended.\")\n",
    "\n",
    "    def dictionary_update(self, Y: np.ndarray):\n",
    "        # iterate rows\n",
    "        D = self.dictionary.matrix\n",
    "        n, K = D.shape\n",
    "        R = Y - D.dot(self.alphas)\n",
    "        for k in range(K):\n",
    "            logging.info(\"Updating column %s\" % k)\n",
    "            wk = np.nonzero(self.alphas[k, :])[0]\n",
    "            if len(wk) == 0:\n",
    "                continue\n",
    "            Ri = R[:,wk] + D[:,k,None].dot(self.alphas[None,k,wk])\n",
    "            U, s, Vh = np.linalg.svd(Ri)\n",
    "            D[:, k] = U[:, 0]\n",
    "            self.alphas[k, wk] = s[0] * Vh[0, :]\n",
    "            R[:, wk] = Ri - D[:,k,None].dot(self.alphas[None,k,wk])\n",
    "        self.dictionary = Dictionary(D)\n",
    "\n",
    "    def fit(self, Y: np.ndarray, iter: int):\n",
    "        for i in range(iter):\n",
    "            logging.info(\"Start iteration %s\" % (i + 1))\n",
    "            self.sparse_coding(Y)\n",
    "            self.dictionary_update(Y)\n",
    "        return self.dictionary, self.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d_Mwprh9HiQY"
   },
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    core, factors = partial_tucker(tl.tensor(image), rank=image.shape, modes=[0, 1, 2])\n",
    "    return factors[0]\n",
    "\n",
    "\n",
    "def get_matrix(y, image_path, patch_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    component = preprocess(image)\n",
    "    \n",
    "    # Extract all reference patches from the image\n",
    "    patches = extract_patches_2d(component, (patch_size, patch_size))\n",
    "    data = patches.reshape(patches.shape[0], -1)\n",
    "    y = np.vstack([y, data])\n",
    "    \n",
    "    return y\n",
    "  \n",
    "image_directory = 'images/images_squares_25'\n",
    "image_paths = glob(os.path.join(image_directory, '*.png'))\n",
    "from random import shuffle\n",
    "shuffle(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = image_paths[:50]\n",
    "patch_size = 4\n",
    "\n",
    "y = np.zeros(patch_size * patch_size)\n",
    "for image_path in image_paths:\n",
    "    y = get_matrix(y, image_path, patch_size)\n",
    "y = np.delete(y, 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AVlhhBoqVVpk"
   },
   "outputs": [],
   "source": [
    "test = y.T\n",
    "u, s, v = np.linalg.svd(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "print(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RuEZnnHvVXsK"
   },
   "outputs": [],
   "source": [
    "initial_dictionary = Dictionary(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_dictionary.is_unitary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksvd = KSVD(initial_dictionary, OrthogonalMatchingPursuit, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZgizTHPxYtxu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start iteration 1\n",
      "INFO:root:Entering sparse coding stage...\n",
      "INFO:root:Sparse coding stage ended.\n",
      "INFO:root:Updating column 0\n",
      "INFO:root:Updating column 1\n",
      "INFO:root:Updating column 2\n",
      "INFO:root:Updating column 3\n",
      "INFO:root:Updating column 4\n",
      "INFO:root:Updating column 5\n",
      "INFO:root:Updating column 6\n",
      "INFO:root:Updating column 7\n",
      "INFO:root:Updating column 8\n",
      "INFO:root:Updating column 9\n",
      "INFO:root:Updating column 10\n",
      "INFO:root:Updating column 11\n",
      "INFO:root:Updating column 12\n",
      "INFO:root:Updating column 13\n",
      "INFO:root:Updating column 14\n",
      "INFO:root:Updating column 15\n"
     ]
    }
   ],
   "source": [
    "# второй аргумент - число итераций\n",
    "learn_dict, coeff = ksvd.fit(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RlKFv2zuS29q"
   },
   "outputs": [],
   "source": [
    "v = learn_dict.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20843785430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD4CAYAAAAO2kjhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAU7ElEQVR4nO3df6yU1Z3H8fdXwE1EqyCKqLSCSzS0/BAI1CyxslUUSgpuDD9qd7mWBiWFlkaDrqSlobVx61pasWkLhYhbtbqrKN1SAYXG1ioKFH+CgIiFK4I/EFGpCP3uH/PczeWemXu/c2funRn8vJLJnfvMZ855hsn98jzznDnH3B0RkcaOq/QOiEj1UWEQkYQKg4gkVBhEJKHCICKJjpXegXzMTJdKRNqYu1uhx3TEICKJkgqDmV1uZi+b2TYzuzHP4/9gZvdlj681s3NK6U9E2kerC4OZdQB+BowC+gKTzKxvk9gUYJ+7/yMwD/iP1vYnIu2nlCOGocA2d9/u7oeA3wBjm2TGAkuy+/8DfNHMCp7XiEh1KKUwnAXsbPT7rmxb3oy7Hwb2A6eW0KeItIOquSphZlOBqZXeDxEp7YihHujZ6Pezs215M2bWETgZeDtfY+6+wN2HuPuQEvZJRMqglMLwDNDHzHqZ2fHARGBZk8wyYHJ2/0pgtevrnCJVr9WnEu5+2MymAyuADsBid3/RzOYC69x9GbAI+C8z2wa8Q654iEiVs2r8Dzw68nHmzJnhNufNmxfKTZs2Ldxm7969Q7lZs2aFcv379w/3PXLkyFDu1ltvDeXGjRsX7vvhhx8O5YYMiZ8VvvXWW6HcG2+8EcrNnTs33Hf0/Yn60pe+FM7Onj07lJsxY0a4zfXr14dyGvkoIkVRYRCRhAqDiCRUGEQkocIgIgkVBhFJqDCISEKFQUQSKgwiklBhEJFE1XztujX27NkTzl5yySWhXDHDWc8///xwNuLNN98MZ6NDiK+88spQbtiwYeG+o0Oi161bF25z586dLYeACRMmhHKnnXZauO+oJ598MpTr1q1buM2PP/44lGuL19McHTGISEKFQUQSKgwiklBhEJGECoOIJFQYRCRRyoIzPc1sjZm9ZGYvmtm38mQuNrP9ZrYxu323tN0VkfZQyjiGw8B17r7BzE4C1pvZKnd/qUnuj+4+poR+RKSdtfqIwd13u/uG7P4BYBPpgjMiUoPKMvIxW6z2AmBtnocvNLNngdeB6939xQJtFL3gzFe/+tVw9qmnngrljhw5Em7zxBNPDGcjTj01vkjX6NGjQ7nx48eHcsVMBhu1cuXKcHbx4sWh3FVXXRXK9erVK9x31F133RXKDR06NNxmXV1dKBd9HwEeeeSRcLaQkguDmZ0IPADMdPf3mjy8AfiMu79vZqOBh4A++dpx9wXAgqzN6pu6WuQTpKSrEmbWiVxRuNvdH2z6uLu/5+7vZ/eXA53MLD6QXEQqopSrEkZuQZlN7v7jApkzGla3NrOhWX95l6gTkepRyqnEPwH/CjxvZhuzbTcBnwZw91+QW5ZumpkdBg4CE7VEnUj1K2WJuj8BBVeyyTJ3AHe0tg8RqQyNfBSRhAqDiCRUGEQkocIgIgkVBhFJ1PRksBs2bCh7m5/61KfC2YsuuqisfRczxDs6eeqIESNCue3bt4f7jrr00kvD2ei/+6ZNm0K5NWvWhPuOOnDgQCh35plnhtucOjX2LYCFCxeG2ywHHTGISEKFQUQSKgwiklBhEJGECoOIJFQYRCShwiAiCRUGEUmoMIhIwqpx3hTN+SjS9ty94HwqOmIQkUTJhcHMdpjZ89lKU+vyPG5mdruZbTOz58xsUKl9ikjbKteXqEa4+1sFHhtFbsr4PsAw4OfZTxGpUu1xKjEWuMtzngJOMbMe7dCviLRSOQqDAyvNbH22mlRTZwGNvyO8izxL2ZnZVDNbl+90RETaVzlOJYa7e72ZnQ6sMrPN7v54sY1oJSqR6lHyEYO712c/9wJLgaYL99UDPRv9fna2TUSqVKlL1HU2s5Ma7gMjgReaxJYB/5Zdnfg8sN/dd5fSr4i0rVJPJboDS7NV6DoC97j7I2Z2Lfz/alTLgdHANuBD4OoS+xSRNvaJGfk4a9asUO6jjz4KtzloUGxIxuTJk0O5RYsWhftetmxZKDdnzpxQLivuIRdccEEo9+CDyTrHBZ122mmh3Ntvx5Y+/fDDD8N9f+UrXwnlOnToEMq9+eab4b5Xr14dyj399NPhNn/0ox+Fchr5KCJFUWEQkYQKg4gkVBhEJKHCICIJFQYRSagwiEhChUFEEioMIpJQYRCRRLlmcKqItWvXhrMbNmwI5Y47Ll4re/bs2XKoCP369Qtnp0yZEsq9+uqrody3v/3tcN9R9fXxL9GecMIJoVx0CH90mHMxjhw5Espde+214TYHDx4cyu3cubPlUBnpiEFEEioMIpJQYRCRhAqDiCRUGEQkocIgIolWFwYzOy9bfarh9p6ZzWySudjM9jfKfLfkPRaRNtfqcQzu/jIwEMDMOpCb+Xlpnugf3X1Ma/sRkfZXrlOJLwKvuPtrZWpPRCqoXCMfJwL3FnjsQjN7FngduN7dX8wXylaxyreSVUFbtmwJZ6Mj5n74wx+G2+zSpUs4G7F///5wNjo5aHSS1SuuuCLcd9T06dPD2XXrYguQ7d27N5SbP39+uO8ZM2aEcg899FAo98QTT4T7jo7QXLhwYbjNcijHatfHA18G/jvPwxuAz7j7AGA+8FChdtx9gbsPcfchpe6TiJSmHKcSo4AN7r6n6QPu/p67v5/dXw50MrNuZehTRNpQOQrDJAqcRpjZGZYtWGBmQ7P+YgsDiEjFlPQZQ7Ys3aXANY22NV6F6kpgmpkdBg4CE70aV7gRkaOUVBjc/QPg1CbbftHo/h3AHaX0ISLtTyMfRSShwiAiCRUGEUmoMIhIoqbnfCxmhFn0YsiOHTvCbX7/+98P5TZu3BjK9erVK9z3ueeeG8otXrw4lBswYEC47+jree21+Aj56IjTmTNnhnLlno8T4qMP6+rqwm1GR1Nu3bo13GY2QqAkOmIQkYQKg4gkVBhEJKHCICIJFQYRSagwiEhChUFEEioMIpJQYRCRhAqDiCSsGudNMbPq2ymRY4y7Fxw7rSMGEUmECoOZLTazvWb2QqNtXc1slZltzX7mnUvdzCZnma1mNrlcOy4ibSd6xHAncHmTbTcCj7l7H+Cx7PejmFlXYA4wDBgKzClUQESkeoQKg7s/DrzTZPNYYEl2fwkwLs9TLwNWufs77r4PWEVaYESkypQyH0N3d9+d3X8D6J4ncxaws9Hvu7JtidasRCUibaMsE7W4u5d6JcHdFwALQFclRCqtlKsSe8ysB0D2M9+igvVA46l0zs62iUgVK6UwLAMarjJMBh7Ok1kBjDSzLtmHjiOzbSJSzdy9xRu5Jeh2Ax+T+5xgCrmFZh4DtgKPAl2z7BDgV42e+zVgW3a7Otif66abbm17a+5vsKZHPl5//fXhNq+55pqWQ0U6ePBgKNe/f/9QbsqUKeG+Bw8eHMr169cvlBs+fHi47+hko6tXrw63Gf03+ulPfxrKRSfqLUanTp1Cueg+AjzwwAOh3KxZs8JtXnbZZaGcRj6KSFFUGEQkocIgIgkVBhFJqDCISEKFQUQSKgwiklBhEJGECoOIJFQYRCRRlq9dV8o3v/nNcPYvf/lLKFfM0NOPPvoonI04fPhwODtu3LhQ7qabbgrlunXrFu67LTz66KOh3NixY0O5OXPmhPvu2DH2Z3Do0KFQrpjh5YMGDQrltmzZEm6zHHTEICIJFQYRSagwiEhChUFEEioMIpJQYRCRRIuFocAqVLea2WYze87MlprZKQWeu8PMnjezjWa2roz7LSJtKHLEcCfpIjGrgM+5e39gC/DvzTx/hLsPdPchrdtFEWlvLRaGfKtQuftKd28YjfMUuWnhReQYUY6Rj18D7ivwmAMrs8ldf5ktKpNXa1aiKmaC19///veh3A033BBuc9q0aaHcOeecE8odd1z8I58xY8aEcrfddlso94UvfCHcd1Qxk6IuW7YslNu3b18oV1dXF+476p577gnlohP1Alx44YWh3IQJE8JtzpgxI5wtpKTCYGazgcPA3QUiw9293sxOB1aZ2ebsCCShlahEqkerr0qYWR0wBrjKC8xB7+712c+9wFJyK16LSJVrVWEws8uBWcCX3f3DApnOZnZSw31yq1C9kC8rItUlcrnyXuBJ4Dwz22VmU4A7gJPInR5sNLNfZNkzzWx59tTuwJ/M7FngaeB37v5Im7wKESmrFj9jcPdJeTYvKpB9HRid3d8ODChp70SkIjTyUUQSKgwiklBhEJGECoOIJKzAEISKig5weuWVV8Jtbt++PZS75JJLwm3+4Ac/COW+853vhHLR0ZkAo0aNCuW6d+8eyg0bNizcd3SU4rvvvhtu8+STTw7lNm/eHMp16dIl3PcZZ5wRyt1yyy2hXH19fbjv+fPnh3IffPBBuM3OnTuHcu5uhR7TEYOIJFQYRCShwiAiCRUGEUmoMIhIQoVBRBIqDCKSUGEQkYQKg4gkVBhEJFHTQ6JFpPU0JFpEitLalai+Z2b12bRuG81sdIHnXm5mL5vZNjO7sZw7LiJtp8VTCTO7CHgfuMvdP5dt+x7wvrv/ZzPP60BulapLgV3AM8Akd3+pxZ3SqYRImyvpVCLfSlRBQ4Ft7r7d3Q8BvwHGtqIdEWlnpXzGMD1b1HaxmeX78vtZwM5Gv+/KtuVlZlPNbJ0WvxWpvNYWhp8D5wIDgd1AbB20Zrj7AncfosVvRSqvVYXB3fe4+xF3/zuwkPwrTNUDPRv9fna2TUSqXGtXourR6NcryL/C1DNAHzPrZWbHAxOB2JxgIlJRLS44k61EdTHQzcx2AXOAi81sILnVrHcA12TZM4Ffuftodz9sZtOBFUAHYLG7v9gWL0JEyqumRz5+/etfD7e5a9euUO7gwYPhNv/whz+EcmYFrwodZeHCheG+P/vZz4Zyc+bMCeXuv//+cN/RiVaLeX8GDIgtWrZ8+fKWQ8D48ePDfV999dWh3M033xzKzZ49O9z32rVrQ7n9+/eH2xw5cmQop5GPIlIUFQYRSagwiEhChUFEEioMIpJQYRCRhAqDiCRUGEQkocIgIokWh0RXsz//+c/hbHRk3a9//etwm6effno4G7Fjx45wtl+/fqFcdEn222+/Pdx3VI8ePVoOZX7729+GckuWLAnl5s6dG+47asWKFaFc3759w20eOnQolHv77bfDbZaDjhhEJKHCICIJFQYRSagwiEhChUFEEioMIpJQYRCRRGRqt8XAGGBvowVn7gPOyyKnAO+6+8A8z90BHACOAIc1A7RIbYgMcLoTuAO4q2GDu09ouG9mtwHNzTs1wt3fau0Oikj7a7EwuPvjZnZOvscsN5nheOCfy7xfIlJBoclgs8Lwvw2nEo22XwT8uNApgpm9CuwjN5v0L919QTN9TAWmZr8Ojuz8lClTIjEAFi1aFMqdcsop4TYHDhwYykUnjS1GdBLfv/71r6HcmjVrwn3X1dWFs1HR4e3RXHQoOMC8efNCueuuuy6Ue+KJJ8J933333aHcCSecEG4zOhS9uclgS/2uxCTg3mYeH+7u9WZ2OrDKzDZna2Hm28kFwALQorYildbqqxJm1hH4F+C+Qhl3r89+7gWWkn/FKhGpMqVcrrwE2OzueRdsMLPOZnZSw31gJPlXrBKRKtNiYchWonoSOM/MdplZw4n9RJqcRpjZmWbWsCJId+BPZvYs8DTwO3d/pHy7LiJtJXJVYlKB7XV5tr0OjM7ubwdikyCISFXRyEcRSagwiEhChUFEEioMIpKo6clgixlh9re//S2UO//888NtduxY3n++Yka3LV26NJTr2bNnKBedlLQY0dGZEF+Kvn///qFc165dw31H7dy5M5Q7cOBAuM3evXuHctFJcMtFRwwiklBhEJGECoOIJFQYRCShwiAiCRUGEUmoMIhIQoVBRBIqDCKSUGEQkURoMtj2pjkfRdpec5PBRmZw6mlma8zsJTN70cy+lW3vamarzGxr9rNLgedPzjJbzWxy61+GiLSXFo8YzKwH0MPdN2RzOK4HxgF1wDvufouZ3Qh0cfcbmjy3K7AOGEJuCvn1wGB339dCnzpiEGljJR0xuPtud9+Q3T8AbALOAsYCDV/5WkKuWDR1GbDK3d/JisEq4PKi9l5E2l1RHz5mC89cAKwFurv77uyhN8hN/trUWUDj76ruyraJSBULTyhgZicCDwAz3f293Op0Oe7upR7+N1mJSkQqKHTEYGadyBWFu939wWzznuzzh4bPIfbmeWo90HimkLOzbQl3X+DuQ7QitkjlRa5KGLAI2OTuP2700DKg4SrDZODhPE9fAYw0sy7ZVYuR2TYRqWbu3uwNGE7uisJzwMbsNho4FXgM2Ao8CnTN8kOAXzV6/teAbdnt6pb6y57juummW9vemvsb1AAnkU+otlztuq28BbzWZFu3bPux4lh6PcfSa4FPxuv5THNPqMojhnzMbN2x9MHksfR6jqXXAno9oC9RiUgeKgwikqilwrCg0jtQZsfS6zmWXgvo9dTOZwwi0n5q6YhBRNqJCoOIJKq+MJjZ5Wb2splty+Z9qGlmtsPMnjezjWa2rtL7UywzW2xme83shUbbQpP2VKMCr+d7ZlafvUcbzWx0JfcxqtRJlRqr6sJgZh2AnwGjgL7AJDPrW9m9KosR7j6wRq+V30k6p8aNwGPu3ofcMPlaKuB3kn+OkHnZezTQ3Ze38z611mHgOnfvC3we+Eb291L0+1PVhQEYCmxz9+3ufgj4DbkJYqRC3P1x4J0mm8fS8qQ9VanA66lJJU6qdJRqLwzH4kQvDqw0s/XZHBTHgsikPbVmupk9l51q1MypUYNWTKp0lGovDMei4e4+iNzp0TfM7KJK71A5ee76d61fA/85cC4wENgN3FbRvSlS00mVGj8WfX+qvTCEJ3qpFe5en/3cCywld7pU6yKT9tQMd9/j7kfc/e/AQmroPSphUqWjVHtheAboY2a9zOx4YCK5CWJqkpl1zmbaxsw6k5u45oXmn1UTIpP21IyGP6LMFdTIe1TipEpHt1XtIx+zS0U/AToAi9395sruUeuZWW9yRwmQ+8r7PbX2eszsXuBicl/l3QPMAR4C7gc+Te7r8uPdvSY+0Cvwei4mdxrhwA7gmkbn6FXLzIYDfwSeB/6ebb6J3OcMRb0/VV8YRKT9VfuphIhUgAqDiCRUGEQkocIgIgkVBhFJqDCISEKFQUQS/wcYE5yi8StingAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(learn_dict.to_img(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вычисление фич\n",
    "def get_features(x):\n",
    "    f_mic = []\n",
    "    f_mac = []\n",
    "    for i in range(x.shape[0]):\n",
    "        values = x[i]\n",
    "        values = np.abs(values[values!=0])\n",
    "        sigma, _, mean = sp.stats.lognorm.fit(values, loc=0)\n",
    "        f_mic.append(np.exp(mean + 0.5*sigma**2))\n",
    "        f_mac.append(values.shape[0])\n",
    "    return f_mic + f_mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем словарь на диск\n",
    "np.save(open('dictionary1.npy', 'wb'), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.load(open('dictionary1.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание таблицы для записи фич\n",
    "df = pd.read_csv('images/response.csv')\n",
    "images = df['image'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "image_paths = ['images/images_squares_25/'+image for image in images]\n",
    "del df\n",
    "image_paths = image_paths[:50]\n",
    "columns = ['image']\n",
    "for i in range(v.shape[1]):\n",
    "    columns.append('f%d_mic'%(i+1))\n",
    "for i in range(v.shape[1]):\n",
    "    columns.append('f%d_mac'%(i+1))\n",
    "columns.append('label')\n",
    "data = {column: [] for column in columns}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhUQoj9iQ4WM",
    "outputId": "41f13552-c10f-4e83-c4fb-dec974b849d9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0/50 images/images_squares_25/image1.png\n",
      "1\n",
      "1\n",
      "1/50 images/images_squares_25/image2.png\n",
      "2\n",
      "2\n",
      "2/50 images/images_squares_25/image3.png\n",
      "3\n",
      "3\n",
      "3/50 images/images_squares_25/image4.png\n",
      "4\n",
      "4\n",
      "4/50 images/images_squares_25/image5.png\n",
      "5\n",
      "5\n",
      "5/50 images/images_squares_25/image6.png\n",
      "6\n",
      "6\n",
      "6/50 images/images_squares_25/image7.png\n",
      "7\n",
      "7\n",
      "7/50 images/images_squares_25/image8.png\n",
      "8\n",
      "8\n",
      "8/50 images/images_squares_25/image9.png\n",
      "9\n",
      "9\n",
      "9/50 images/images_squares_25/image10.png\n",
      "10\n",
      "10\n",
      "10/50 images/images_squares_25/image11.png\n",
      "11\n",
      "11\n",
      "11/50 images/images_squares_25/image12.png\n",
      "12\n",
      "12\n",
      "12/50 images/images_squares_25/image13.png\n",
      "13\n",
      "13\n",
      "13/50 images/images_squares_25/image14.png\n",
      "14\n",
      "14\n",
      "14/50 images/images_squares_25/image15.png\n",
      "15\n",
      "15\n",
      "15/50 images/images_squares_25/image16.png\n",
      "16\n",
      "16\n",
      "16/50 images/images_squares_25/image17.png\n",
      "17\n",
      "17\n",
      "17/50 images/images_squares_25/image18.png\n",
      "18\n",
      "18\n",
      "18/50 images/images_squares_25/image19.png\n",
      "19\n",
      "19\n",
      "19/50 images/images_squares_25/image20.png\n",
      "20\n",
      "20\n",
      "20/50 images/images_squares_25/image21.png\n",
      "21\n",
      "21\n",
      "21/50 images/images_squares_25/image22.png\n",
      "22\n",
      "22\n",
      "22/50 images/images_squares_25/image23.png\n",
      "23\n",
      "23\n",
      "23/50 images/images_squares_25/image24.png\n",
      "24\n",
      "24\n",
      "24/50 images/images_squares_25/image25.png\n",
      "25\n",
      "25\n",
      "25/50 images/images_squares_25/image26.png\n",
      "26\n",
      "26\n",
      "26/50 images/images_squares_25/image27.png\n",
      "27\n",
      "27\n",
      "27/50 images/images_squares_25/image28.png\n",
      "28\n",
      "28\n",
      "28/50 images/images_squares_25/image29.png\n",
      "29\n",
      "29\n",
      "29/50 images/images_squares_25/image30.png\n",
      "30\n",
      "30\n",
      "30/50 images/images_squares_25/image31.png\n",
      "31\n",
      "31\n",
      "31/50 images/images_squares_25/image32.png\n",
      "32\n",
      "32\n",
      "32/50 images/images_squares_25/image33.png\n",
      "33\n",
      "33\n",
      "33/50 images/images_squares_25/image34.png\n",
      "34\n",
      "34\n",
      "34/50 images/images_squares_25/image35.png\n",
      "35\n",
      "35\n",
      "35/50 images/images_squares_25/image36.png\n",
      "36\n",
      "36\n",
      "36/50 images/images_squares_25/image37.png\n",
      "37\n",
      "37\n",
      "37/50 images/images_squares_25/image38.png\n",
      "38\n",
      "38\n",
      "38/50 images/images_squares_25/image39.png\n",
      "39\n",
      "39\n",
      "39/50 images/images_squares_25/image40.png\n",
      "40\n",
      "40\n",
      "40/50 images/images_squares_25/image41.png\n",
      "41\n",
      "41\n",
      "41/50 images/images_squares_25/image42.png\n",
      "42\n",
      "42\n",
      "42/50 images/images_squares_25/image43.png\n",
      "43\n",
      "43\n",
      "43/50 images/images_squares_25/image44.png\n",
      "44\n",
      "44\n",
      "44/50 images/images_squares_25/image45.png\n",
      "45\n",
      "45\n",
      "45/50 images/images_squares_25/image46.png\n",
      "46\n",
      "46\n",
      "46/50 images/images_squares_25/image47.png\n",
      "47\n",
      "47\n",
      "47/50 images/images_squares_25/image48.png\n",
      "48\n",
      "48\n",
      "48/50 images/images_squares_25/image49.png\n",
      "49\n",
      "49\n",
      "49/50 images/images_squares_25/image50.png\n"
     ]
    }
   ],
   "source": [
    "for i, image_path in enumerate(image_paths):\n",
    "    y = np.zeros(patch_size * patch_size)\n",
    "    y = get_matrix(y, image_path, patch_size)\n",
    "    y = y.T\n",
    "    print(i)\n",
    "    X = orthogonal_mp(v, y, n_nonzero_coefs=8)\n",
    "    print(i)\n",
    "    features = get_features(X)\n",
    "    data = [images[i]] + features + [labels[i]]\n",
    "    data = {column: [datum] for column, datum in zip(columns, data)}\n",
    "    tmp = pd.DataFrame(data)\n",
    "    df = pd.concat([df, tmp], axis=0)\n",
    "    print('%d/%d %s' % (i, len(image_paths), image_path))\n",
    "\n",
    "df.to_csv('dataset2_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без понижения размерности признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset2_1.csv')\n",
    "columns = [column for column in df.columns if column not in ['image', 'label']]\n",
    "x = df[columns].to_numpy()\n",
    "y = df['label'].to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8)\n",
    "model = make_pipeline(StandardScaler(), SVR(C=35, epsilon=0.1))\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9273459117928695"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(abs(y_pred-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0250276955676005"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, y_test, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # for data visualization\n",
    "import pandas as pd # for data analysis\n",
    "import numpy as np # for numeric calculation\n",
    "import matplotlib.pyplot as plt # for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv('dataset2.csv')\n",
    "df = df.drop('image', axis=1)\n",
    "df = df.drop('label', axis=1)\n",
    "# get correlations\n",
    "df_corr = df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,100))\n",
    "# plot heatmap\n",
    "sns.heatmap(df_corr, annot=True, fmt=\".2f\", cmap='Blues',\n",
    "           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "df = pd.read_csv('dataset2.csv')\n",
    "df = df.drop('image', axis=1)\n",
    "df = df.drop('label', axis=1)\n",
    "def calculate_pvalues(df):\n",
    "    df = df.dropna()._get_numeric_data()\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)\n",
    "    return pvalues\n",
    "pvalues = calculate_pvalues(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with significance filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# get correlation\n",
    "corr = df.corr(method='pearson')     \n",
    "def corr_sig(df=None):\n",
    "    p_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    s_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    for col in df.columns:\n",
    "        for col2 in df.drop(col,axis=1).columns:\n",
    "            s , p = stats.pearsonr(df[col],df[col2])\n",
    "            p_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = p\n",
    "            s_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = s\n",
    "    return p_matrix, s_matrix\n",
    "p_values, corr_ = corr_sig(df)                     # get p-Value\n",
    "mask = np.tril(p_values<0.05)    # mask - only get significant corr\n",
    "def plot_cor_matrix(corr, mask=None):\n",
    "    f, ax = plt.subplots(figsize=(200, 200))\n",
    "    sns.heatmap(corr, ax=ax,\n",
    "                mask=mask,\n",
    "                # cosmetics\n",
    "                annot=True, vmin=-1, vmax=1, center=0,\n",
    "                cmap='Blues', linewidths=2, linecolor='black')\n",
    "plot_cor_matrix(corr_,mask)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(method='pearson')     \n",
    "def corr_sig(df=None):\n",
    "    p_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    s_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    for col in df.columns:\n",
    "        for col2 in df.drop(col,axis=1).columns:\n",
    "            s , p = stats.pearsonr(df[col],df[col2])\n",
    "            p_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = p\n",
    "            s_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = s\n",
    "    return p_matrix, s_matrix\n",
    "p_values, corr_ = corr_sig(df)                     # get p-Value\n",
    "mask = np.invert(np.tril(p_values<0.05))    # mask - only get significant corr\n",
    "def plot_cor_matrix(corr, mask=None):\n",
    "    f, ax = plt.subplots(figsize=(200, 200))\n",
    "    sns.heatmap(corr, ax=ax,\n",
    "                mask=mask,\n",
    "                # cosmetics\n",
    "                annot=True, vmin=-1, vmax=1, center=0,\n",
    "                cmap='Blues', linewidths=2, linecolor='black')\n",
    "plot_cor_matrix(corr,mask)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.drop('image', axis=1)\n",
    "df = df.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(method='pearson')     \n",
    "def corr_sig(df=None):\n",
    "    p_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    s_matrix = np.zeros(shape=(df.shape[1],df.shape[1]))\n",
    "    for col in df.columns:\n",
    "        for col2 in df.drop(col,axis=1).columns:\n",
    "            s , p = stats.pearsonr(df[col],df[col2])\n",
    "            p_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = p\n",
    "            s_matrix[df.columns.to_list().index(col),df.columns.to_list().index(col2)] = s\n",
    "    return p_matrix, s_matrix\n",
    "p_values, corr_ = corr_sig(df)                     # get p-Value\n",
    "mask = np.invert(np.tril(p_values<0.05))    # mask - only get significant corr\n",
    "def plot_cor_matrix(corr, mask=None):\n",
    "    f, ax = plt.subplots(figsize=(200, 200))\n",
    "    sns.heatmap(corr, ax=ax,\n",
    "                mask=mask,\n",
    "                # cosmetics\n",
    "                annot=True, vmin=-1, vmax=1, center=0,\n",
    "                cmap='Blues', linewidths=2, linecolor='black')\n",
    "plot_cor_matrix(corr,mask)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "columns = [column for column in df.columns if column not in ['image', 'label']]\n",
    "x = df[columns].to_numpy()\n",
    "y = df['label'].to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8)\n",
    "model = make_pipeline(StandardScaler(), SVR(C=35, epsilon=0.1))\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(abs(y_pred-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset2.csv')\n",
    "columns = [column for column in df.columns if column not in ['image', 'label']]\n",
    "x = df[columns].to_numpy()\n",
    "y = df['label'].to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8)\n",
    "model = make_pipeline(StandardScaler(), SVR(C=35, epsilon=0.1))\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.evaluate import feature_importance_permutation\n",
    "\n",
    "print('Training accuracy:', np.mean(model.predict(x_train) == y_train)*100)\n",
    "print('Test accuracy:', np.mean(model.predict(x_test) == y_test)*100)\n",
    "\n",
    "importance_vals = model.feature_importances_\n",
    "print(importance_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "feature_names = [i for i in df.columns if df[i].dtype in [np.int64]]\n",
    "perm = PermutationImportance(model, random_state=1).fit(x_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = x_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'max_depth': 7, 'random_state': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "premu_train = permutation_importance(model, x_train, y_train, n_repeats=30, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [premu_train]\n",
    "target_col = ['label', 'image']\n",
    "n_cols = len(df.columns)\n",
    "\n",
    "x = df.drop(target_col, axis=1)\n",
    "\n",
    "names = [ \n",
    "    'Support Vector Machine'\n",
    "    ]\n",
    "\n",
    "graph_data = {}\n",
    "for result, name in zip(results, names):\n",
    "    graph_data[name] = result['importances_mean']\n",
    "\n",
    "graph_data = pd.DataFrame.from_dict(graph_data, orient='index', columns=x.columns)\n",
    "graph_data.reset_index(inplace=True, drop=False)\n",
    "graph_data.rename(columns={'index': 'model_name'}, inplace=True)\n",
    "graph_data = graph_data.melt(id_vars='model_name')\n",
    "\n",
    "plt.figure(figsize=[350,100])\n",
    "plt.axhline(0, c='black')\n",
    "[plt.axvline(i + 0.5, linestyle='--', c='black') for i in range(0, n_cols)]\n",
    "sns.barplot(x=graph_data['variable'], y=graph_data['value'], hue=graph_data['model_name'])\n",
    "plt.title(\"Permutation Feature Importance Across Models\")\n",
    "plt.xlabel(\"Variable Name\")\n",
    "plt.ylabel(\"Change in Mean Squared Error\")\n",
    "plt.savefig('permutation_importance.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Понижение размерности признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_feature = []\n",
    "for index, row in graph_data.iterrows():\n",
    "    if row['value'] > 0.003:\n",
    "        list_feature.append(row['variable'])\n",
    "print(len(list_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset2.csv')\n",
    "columns = list_feature\n",
    "x = df[columns].to_numpy()\n",
    "y = df['label'].to_numpy()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8)\n",
    "model = make_pipeline(StandardScaler(), SVR(C=35, epsilon=0.1))\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(abs(y_pred-y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform permutation importance\n",
    "results = permutation_importance(model, x_train, y_train, scoring='neg_mean_squared_error')\n",
    "# get importance\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
